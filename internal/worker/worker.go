package worker

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"
	"sync"
	"time"

	"github.com/joshjon/kit/log"
)

const workTypeEpic = "epic"

// Config holds the worker configuration
type Config struct {
	APIURL               string
	AnthropicAPIKey      string // API key auth (pay-per-use)
	ClaudeCodeOAuthToken string // OAuth token auth (subscription-based, alternative to API key)
	AgentImage           string // Docker image for agent - defaults to verve-agent:latest
	MaxConcurrentTasks   int    // Maximum concurrent tasks (default: 1)
	DryRun               bool   // Skip Claude and make a dummy change instead
}

type Task struct {
	ID                 string   `json:"id"`
	RepoID             string   `json:"repo_id"`
	Title              string   `json:"title"`
	Description        string   `json:"description"`
	Status             string   `json:"status"`
	Attempt            int      `json:"attempt"`
	MaxAttempts        int      `json:"max_attempts"`
	RetryReason        string   `json:"retry_reason,omitempty"`
	AcceptanceCriteria []string `json:"acceptance_criteria"`
	RetryContext       string   `json:"retry_context,omitempty"`
	AgentStatus        string   `json:"agent_status,omitempty"`
	CostUSD            float64  `json:"cost_usd"`
	MaxCostUSD         float64  `json:"max_cost_usd,omitempty"`
	SkipPR             bool     `json:"skip_pr"`
	Model              string   `json:"model,omitempty"`
}

type Epic struct {
	ID             string `json:"id"`
	RepoID         string `json:"repo_id"`
	Title          string `json:"title"`
	Description    string `json:"description"`
	PlanningPrompt string `json:"planning_prompt,omitempty"`
	Model          string `json:"model,omitempty"`
}

// PollResponse is a discriminated union returned by the unified poll endpoint.
type PollResponse struct {
	Type         string `json:"type"` // "task" or "epic"
	Task         *Task  `json:"task,omitempty"`
	Epic         *Epic  `json:"epic,omitempty"`
	GitHubToken  string `json:"github_token"`
	RepoFullName string `json:"repo_full_name"`
}

type Worker struct {
	config       Config
	docker       *DockerRunner
	client       *http.Client
	logger       log.Logger
	pollInterval time.Duration

	// Concurrency control
	maxConcurrent int
	semaphore     chan struct{}
	wg            sync.WaitGroup
	activeTasks   int
	activeMu      sync.Mutex
}

func New(cfg Config, logger log.Logger) (*Worker, error) {
	docker, err := NewDockerRunner(cfg.AgentImage, logger)
	if err != nil {
		return nil, err
	}

	// Default to 1 concurrent task if not specified
	maxConcurrent := cfg.MaxConcurrentTasks
	if maxConcurrent <= 0 {
		maxConcurrent = 1
	}

	return &Worker{
		config:        cfg,
		docker:        docker,
		client:        &http.Client{Timeout: 60 * time.Second},
		logger:        logger,
		pollInterval:  5 * time.Second,
		maxConcurrent: maxConcurrent,
		semaphore:     make(chan struct{}, maxConcurrent),
	}, nil
}

func (w *Worker) Close() error {
	return w.docker.Close()
}

func (w *Worker) Run(ctx context.Context) error {
	w.logger.Info("worker starting", "max_concurrent", w.maxConcurrent)

	// Warn if API URL is not HTTPS (tokens will be sent in plaintext)
	if !strings.HasPrefix(w.config.APIURL, "https://") {
		w.logger.Warn("API URL is not HTTPS — GitHub tokens will be sent in plaintext, use HTTPS in production", "api_url", w.config.APIURL)
	}

	// Ensure agent image exists
	if err := w.docker.EnsureImage(ctx); err != nil {
		return err
	}
	w.logger.Info("agent image verified", "image", w.docker.AgentImage())

	for {
		select {
		case <-ctx.Done():
			w.logger.Info("worker shutting down, waiting for active tasks")
			w.wg.Wait()
			w.logger.Info("all tasks completed, worker stopped")
			return ctx.Err()
		default:
		}

		// Try to acquire a semaphore slot (non-blocking check first)
		select {
		case w.semaphore <- struct{}{}:
			// Got a slot, proceed to poll
		default:
			// All slots full, wait a bit before checking again
			time.Sleep(100 * time.Millisecond)
			continue
		}

		poll, err := w.poll(ctx)
		if err != nil {
			// Release slot on error
			<-w.semaphore
			w.logger.Error("error polling for work", "error", err)
			time.Sleep(w.pollInterval)
			continue
		}

		if poll == nil {
			// No work available, release slot and continue polling
			<-w.semaphore
			continue
		}

		// Track active count for logging
		w.activeMu.Lock()
		w.activeTasks++
		activeCount := w.activeTasks
		w.activeMu.Unlock()

		// Dispatch based on work type
		executeFunc := func(p *PollResponse) {
			switch p.Type {
			case workTypeEpic:
				w.logger.Info("claimed epic",
					"epic_id", p.Epic.ID,
					"repo", p.RepoFullName,
					"active", activeCount,
					"title", p.Epic.Title,
				)
				w.executeEpicPlanning(ctx, p.Epic, p.GitHubToken, p.RepoFullName)
			default:
				w.logger.Info("claimed task",
					"task_id", p.Task.ID,
					"repo", p.RepoFullName,
					"active", activeCount,
					"max_concurrent", w.maxConcurrent,
					"description", p.Task.Description,
				)
				w.executeTask(ctx, p.Task, p.GitHubToken, p.RepoFullName)
			}
		}

		if w.maxConcurrent > 1 {
			w.wg.Add(1)
			go func(p *PollResponse) {
				defer w.wg.Done()
				defer func() {
					<-w.semaphore
					w.activeMu.Lock()
					w.activeTasks--
					w.activeMu.Unlock()
				}()
				executeFunc(p)
			}(poll)
		} else {
			executeFunc(poll)
			<-w.semaphore
			w.activeMu.Lock()
			w.activeTasks--
			w.activeMu.Unlock()
		}
	}
}

func (w *Worker) poll(ctx context.Context) (*PollResponse, error) {
	pollURL := w.config.APIURL + "/api/v1/agent/poll"

	req, err := http.NewRequestWithContext(ctx, http.MethodGet, pollURL, http.NoBody)
	if err != nil {
		return nil, err
	}

	resp, err := w.client.Do(req)
	if err != nil {
		return nil, err
	}
	defer func() { _ = resp.Body.Close() }()

	if resp.StatusCode == http.StatusNoContent {
		return nil, nil
	}

	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		return nil, fmt.Errorf("unexpected status %d: %s", resp.StatusCode, body)
	}

	var poll PollResponse
	if err := json.NewDecoder(resp.Body).Decode(&poll); err != nil {
		return nil, err
	}

	return &poll, nil
}

// logStreamer buffers log lines and periodically sends them to the API server
type logStreamer struct {
	worker    *Worker
	taskID    string
	epicID    string
	attempt   int
	ctx       context.Context
	buffer    []string
	mu        sync.Mutex
	done      chan struct{}
	flushed   chan struct{}
	interval  time.Duration
	batchSize int
}

func newLogStreamer(ctx context.Context, w *Worker, taskID string, attempt int) *logStreamer {
	ls := &logStreamer{
		worker:    w,
		taskID:    taskID,
		attempt:   attempt,
		ctx:       ctx,
		buffer:    make([]string, 0, 100),
		done:      make(chan struct{}),
		flushed:   make(chan struct{}),
		interval:  2 * time.Second,
		batchSize: 50,
	}
	go ls.flushLoop()
	return ls
}

func newEpicLogStreamer(ctx context.Context, w *Worker, epicID string) *logStreamer {
	ls := &logStreamer{
		worker:    w,
		epicID:    epicID,
		ctx:       ctx,
		buffer:    make([]string, 0, 100),
		done:      make(chan struct{}),
		flushed:   make(chan struct{}),
		interval:  2 * time.Second,
		batchSize: 50,
	}
	go ls.flushLoop()
	return ls
}

// AddLine adds a log line to the buffer (thread-safe)
func (ls *logStreamer) AddLine(line string) {
	ls.mu.Lock()
	ls.buffer = append(ls.buffer, line)
	shouldFlush := len(ls.buffer) >= ls.batchSize
	ls.mu.Unlock()

	// Flush immediately if buffer is large
	if shouldFlush {
		ls.flush()
	}
}

// Stop signals the streamer to stop and waits for final flush
func (ls *logStreamer) Stop() {
	close(ls.done)
	<-ls.flushed
}

func (ls *logStreamer) flushLoop() {
	defer close(ls.flushed)

	ticker := time.NewTicker(ls.interval)
	defer ticker.Stop()

	for {
		select {
		case <-ticker.C:
			ls.flush()
		case <-ls.done:
			// Final flush
			ls.flush()
			return
		}
	}
}

func (ls *logStreamer) flush() {
	ls.mu.Lock()
	if len(ls.buffer) == 0 {
		ls.mu.Unlock()
		return
	}
	// Take ownership of the buffer
	toSend := ls.buffer
	ls.buffer = make([]string, 0, 100)
	ls.mu.Unlock()

	// Send to API server
	if ls.taskID != "" {
		if err := ls.worker.sendLogs(ls.ctx, ls.taskID, ls.attempt, toSend); err != nil {
			ls.worker.logger.Error("failed to send logs", "task_id", ls.taskID, "error", err)
		}
	} else if ls.epicID != "" {
		if err := ls.worker.sendEpicLogs(ls.ctx, ls.epicID, toSend); err != nil {
			ls.worker.logger.Error("failed to send epic logs", "epic_id", ls.epicID, "error", err)
		}
	}
}

func (w *Worker) executeTask(ctx context.Context, task *Task, githubToken, repoFullName string) {
	taskLogger := w.logger.With("task_id", task.ID)

	// Create log streamer for real-time log streaming
	streamer := newLogStreamer(ctx, w, task.ID, task.Attempt)

	// Track PR info, branch info, and agent markers
	var prURL string
	var prNumber int
	var branchName string
	var agentStatus string
	var costUSD float64
	var prereqFailed string
	var noChanges bool
	var rateLimited bool
	var markerMu sync.Mutex

	// Log callback - called from Docker log streaming goroutine
	onLog := func(line string) {
		taskLogger.Debug("agent output", "line", line)
		streamer.AddLine(line)

		// Strip markdown formatting (e.g. **bold**) that the agent
		// may wrap around marker lines.
		cleanLine := strings.TrimRight(strings.TrimLeft(line, "*"), "*")

		// Parse PR marker
		if strings.HasPrefix(cleanLine, "VERVE_PR_CREATED:") {
			jsonStr := strings.TrimPrefix(cleanLine, "VERVE_PR_CREATED:")
			var prInfo struct {
				URL    string `json:"url"`
				Number int    `json:"number"`
			}
			if err := json.Unmarshal([]byte(jsonStr), &prInfo); err == nil {
				markerMu.Lock()
				prURL = prInfo.URL
				prNumber = prInfo.Number
				markerMu.Unlock()
				taskLogger.Info("captured PR", "url", prURL, "number", prNumber)
			}
		}

		// Parse PR updated marker (retry with existing PR)
		if strings.HasPrefix(cleanLine, "VERVE_PR_UPDATED:") {
			jsonStr := strings.TrimPrefix(cleanLine, "VERVE_PR_UPDATED:")
			var prInfo struct {
				URL    string `json:"url"`
				Number int    `json:"number"`
			}
			if err := json.Unmarshal([]byte(jsonStr), &prInfo); err == nil {
				markerMu.Lock()
				prURL = prInfo.URL
				prNumber = prInfo.Number
				markerMu.Unlock()
				taskLogger.Info("captured PR update", "url", prURL, "number", prNumber)
			}
		}

		// Parse branch pushed marker (skip-PR mode)
		if strings.HasPrefix(cleanLine, "VERVE_BRANCH_PUSHED:") {
			jsonStr := strings.TrimPrefix(cleanLine, "VERVE_BRANCH_PUSHED:")
			var branchInfo struct {
				Branch string `json:"branch"`
			}
			if err := json.Unmarshal([]byte(jsonStr), &branchInfo); err == nil {
				markerMu.Lock()
				branchName = branchInfo.Branch
				markerMu.Unlock()
				taskLogger.Info("captured branch", "branch", branchName)
			}
		}

		// Parse agent status marker
		if strings.HasPrefix(cleanLine, "VERVE_STATUS:") {
			statusJSON := strings.TrimPrefix(cleanLine, "VERVE_STATUS:")
			markerMu.Lock()
			agentStatus = statusJSON
			markerMu.Unlock()
			taskLogger.Info("captured agent status")
		}

		// Parse no-changes marker
		if strings.HasPrefix(cleanLine, "VERVE_NO_CHANGES:") {
			markerMu.Lock()
			noChanges = true
			markerMu.Unlock()
			taskLogger.Info("agent reported no changes needed")
		}

		// Parse prereq failure marker
		if strings.HasPrefix(cleanLine, "VERVE_PREREQ_FAILED:") {
			jsonStr := strings.TrimPrefix(cleanLine, "VERVE_PREREQ_FAILED:")
			markerMu.Lock()
			prereqFailed = jsonStr
			markerMu.Unlock()
			taskLogger.Warn("prerequisite check failed", "details", jsonStr)
		}

		// Parse cost marker
		if strings.HasPrefix(cleanLine, "VERVE_COST:") {
			costStr := strings.TrimPrefix(cleanLine, "VERVE_COST:")
			var cost float64
			if _, err := fmt.Sscanf(costStr, "%f", &cost); err == nil {
				markerMu.Lock()
				costUSD = cost
				markerMu.Unlock()
				taskLogger.Info("captured cost", "cost_usd", cost)
			}
		}

		// Detect Claude rate limit or session max usage errors
		if isRateLimitError(line) {
			markerMu.Lock()
			rateLimited = true
			markerMu.Unlock()
			taskLogger.Warn("detected Claude rate limit or max usage error")
		}
	}

	// Create agent config from worker config + server-provided credentials
	agentCfg := AgentConfig{
		WorkType:             "task",
		TaskID:               task.ID,
		TaskTitle:            task.Title,
		TaskDescription:      task.Description,
		GitHubToken:          githubToken,
		GitHubRepo:           repoFullName,
		AnthropicAPIKey:      w.config.AnthropicAPIKey,
		ClaudeCodeOAuthToken: w.config.ClaudeCodeOAuthToken,
		ClaudeModel:          task.Model,
		DryRun:               w.config.DryRun,
		SkipPR:               task.SkipPR,
		Attempt:              task.Attempt,
		RetryReason:          task.RetryReason,
		AcceptanceCriteria:   task.AcceptanceCriteria,
		RetryContext:         task.RetryContext,
		PreviousStatus:       task.AgentStatus,
	}

	// Start heartbeat goroutine
	heartbeatCtx, cancelHeartbeat := context.WithCancel(ctx)
	defer cancelHeartbeat()
	go w.taskHeartbeatLoop(heartbeatCtx, task.ID)

	// Run the agent with streaming logs
	result := w.docker.RunAgent(ctx, agentCfg, onLog)

	// Stop heartbeat before completing the task
	cancelHeartbeat()

	// Stop the streamer and flush remaining logs
	streamer.Stop()

	// Get captured marker values
	markerMu.Lock()
	capturedPRURL := prURL
	capturedPRNumber := prNumber
	capturedBranchName := branchName
	capturedAgentStatus := agentStatus
	capturedCostUSD := costUSD
	capturedPrereqFailed := prereqFailed
	capturedNoChanges := noChanges
	capturedRateLimited := rateLimited
	markerMu.Unlock()

	// Report completion with PR info, agent status, and cost
	switch {
	case result.Error != nil:
		taskLogger.Error("task failed", "error", result.Error)
		_ = w.completeTask(ctx, task.ID, false, result.Error.Error(), "", 0, "", capturedAgentStatus, capturedCostUSD, capturedPrereqFailed, false, capturedRateLimited)
	case result.Success:
		if capturedNoChanges {
			taskLogger.Info("task completed — no changes needed")
		} else {
			taskLogger.Info("task completed successfully")
		}
		_ = w.completeTask(ctx, task.ID, true, "", capturedPRURL, capturedPRNumber, capturedBranchName, capturedAgentStatus, capturedCostUSD, "", capturedNoChanges, false)
	default:
		errMsg := fmt.Sprintf("exit code %d", result.ExitCode)
		if capturedPrereqFailed != "" {
			errMsg = "prerequisite check failed"
		}
		taskLogger.Error("task failed", "exit_code", result.ExitCode)
		_ = w.completeTask(ctx, task.ID, false, errMsg, "", 0, "", capturedAgentStatus, capturedCostUSD, capturedPrereqFailed, false, capturedRateLimited)
	}
}

func (w *Worker) executeEpicPlanning(ctx context.Context, ep *Epic, githubToken, repoFullName string) {
	epicLogger := w.logger.With("epic_id", ep.ID)
	epicLogger.Info("starting epic planning", "title", ep.Title)

	// Create log streamer for real-time log streaming
	streamer := newEpicLogStreamer(ctx, w, ep.ID)

	agentCfg := AgentConfig{
		WorkType:             workTypeEpic,
		EpicID:               ep.ID,
		EpicTitle:            ep.Title,
		EpicDescription:      ep.Description,
		EpicPlanningPrompt:   ep.PlanningPrompt,
		APIURL:               w.config.APIURL,
		GitHubToken:          githubToken,
		GitHubRepo:           repoFullName,
		AnthropicAPIKey:      w.config.AnthropicAPIKey,
		ClaudeCodeOAuthToken: w.config.ClaudeCodeOAuthToken,
		ClaudeModel:          ep.Model,
	}

	// Start heartbeat goroutine
	heartbeatCtx, cancelHeartbeat := context.WithCancel(ctx)
	defer cancelHeartbeat()
	go w.epicHeartbeatLoop(heartbeatCtx, ep.ID)

	// Log callback for epic planning — log at INFO level so agent output
	// is visible in worker logs (helps diagnose issues)
	onLog := func(line string) {
		epicLogger.Info("epic agent", "line", line)
		streamer.AddLine(line)
	}

	result := w.docker.RunAgent(ctx, agentCfg, onLog)

	// Stop heartbeat before completing
	cancelHeartbeat()

	// Stop the streamer and flush remaining logs
	streamer.Stop()

	switch {
	case result.Error != nil:
		epicLogger.Error("epic planning failed", "error", result.Error)
	case result.Success:
		epicLogger.Info("epic planning container exited successfully")
	default:
		epicLogger.Error("epic planning container failed", "exit_code", result.ExitCode)
	}
}

func (w *Worker) sendLogs(ctx context.Context, taskID string, attempt int, logs []string) error {
	body, _ := json.Marshal(map[string]any{"logs": logs, "attempt": attempt})
	req, err := http.NewRequestWithContext(ctx, http.MethodPost, w.config.APIURL+"/api/v1/agent/tasks/"+taskID+"/logs", bytes.NewReader(body))
	if err != nil {
		return err
	}
	req.Header.Set("Content-Type", "application/json")

	resp, err := w.client.Do(req)
	if err != nil {
		return err
	}
	defer func() { _ = resp.Body.Close() }()

	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		return fmt.Errorf("unexpected status %d: %s", resp.StatusCode, body)
	}
	return nil
}

func (w *Worker) sendEpicLogs(ctx context.Context, epicID string, logs []string) error {
	body, _ := json.Marshal(map[string]any{"lines": logs})
	req, err := http.NewRequestWithContext(ctx, http.MethodPost, w.config.APIURL+"/api/v1/agent/epics/"+epicID+"/logs", bytes.NewReader(body))
	if err != nil {
		return err
	}
	req.Header.Set("Content-Type", "application/json")

	resp, err := w.client.Do(req)
	if err != nil {
		return err
	}
	defer func() { _ = resp.Body.Close() }()

	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		return fmt.Errorf("unexpected status %d: %s", resp.StatusCode, body)
	}
	return nil
}

func (w *Worker) taskHeartbeatLoop(ctx context.Context, taskID string) {
	ticker := time.NewTicker(30 * time.Second)
	defer ticker.Stop()

	// Send initial heartbeat immediately
	_ = w.sendTaskHeartbeat(ctx, taskID)

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			_ = w.sendTaskHeartbeat(ctx, taskID)
		}
	}
}

func (w *Worker) sendTaskHeartbeat(ctx context.Context, taskID string) error {
	req, err := http.NewRequestWithContext(ctx, http.MethodPost,
		w.config.APIURL+"/api/v1/agent/tasks/"+taskID+"/heartbeat", http.NoBody)
	if err != nil {
		return err
	}

	resp, err := w.client.Do(req)
	if err != nil {
		return err
	}
	_ = resp.Body.Close()
	return nil
}

func (w *Worker) epicHeartbeatLoop(ctx context.Context, epicID string) {
	ticker := time.NewTicker(30 * time.Second)
	defer ticker.Stop()

	_ = w.sendEpicHeartbeat(ctx, epicID)

	for {
		select {
		case <-ctx.Done():
			return
		case <-ticker.C:
			_ = w.sendEpicHeartbeat(ctx, epicID)
		}
	}
}

func (w *Worker) sendEpicHeartbeat(ctx context.Context, epicID string) error {
	req, err := http.NewRequestWithContext(ctx, http.MethodPost,
		w.config.APIURL+"/api/v1/agent/epics/"+epicID+"/heartbeat", http.NoBody)
	if err != nil {
		return err
	}

	resp, err := w.client.Do(req)
	if err != nil {
		return err
	}
	_ = resp.Body.Close()
	return nil
}

func (w *Worker) completeTask(ctx context.Context, taskID string, success bool, errMsg, prURL string, prNumber int, branchName, agentStatus string, costUSD float64, prereqFailed string, noChanges, retryable bool) error {
	payload := map[string]interface{}{"success": success}
	if errMsg != "" {
		payload["error"] = errMsg
	}
	if prURL != "" {
		payload["pull_request_url"] = prURL
		payload["pr_number"] = prNumber
	}
	if branchName != "" {
		payload["branch_name"] = branchName
	}
	if agentStatus != "" {
		payload["agent_status"] = agentStatus
	}
	if costUSD > 0 {
		payload["cost_usd"] = costUSD
	}
	if prereqFailed != "" {
		payload["prereq_failed"] = prereqFailed
	}
	if noChanges {
		payload["no_changes"] = true
	}
	if retryable {
		payload["retryable"] = true
	}
	body, _ := json.Marshal(payload)

	req, err := http.NewRequestWithContext(ctx, http.MethodPost, w.config.APIURL+"/api/v1/agent/tasks/"+taskID+"/complete", bytes.NewReader(body))
	if err != nil {
		return err
	}
	req.Header.Set("Content-Type", "application/json")

	resp, err := w.client.Do(req)
	if err != nil {
		return err
	}
	defer func() { _ = resp.Body.Close() }()

	if resp.StatusCode != http.StatusOK {
		body, _ := io.ReadAll(resp.Body)
		return fmt.Errorf("unexpected status %d: %s", resp.StatusCode, body)
	}
	return nil
}

// rateLimitPatterns are substrings in agent output that indicate Claude rate
// limit or session max usage errors. These are transient and the task should
// be retried after a delay rather than permanently failed.
var rateLimitPatterns = []string{
	"max usage",
	"rate limit",
	"rate_limit",
	"too many requests",
	"overloaded_error",
}

// isRateLimitError checks if a log line indicates a Claude rate limit or
// session max usage error.
func isRateLimitError(line string) bool {
	lower := strings.ToLower(line)
	for _, pattern := range rateLimitPatterns {
		if strings.Contains(lower, pattern) {
			return true
		}
	}
	return false
}
